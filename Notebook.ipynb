{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "MLClassifier.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOqBwipoHDzT5Uv76Xbx1La"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1WEX0BsYhf-"
      },
      "source": [
        "# Welcome\n",
        "\n",
        "In this notebook we are going to train our own [Deep Neural Network (DNN)](https://en.wikipedia.org/wiki/Deep_learning).\n",
        "\n",
        "With our DNN we are going to be solving a [Categorical Classification](https://en.wikipedia.org/wiki/Statistical_classification) problem in the [Computer Vision](https://en.wikipedia.org/wiki/Computer_vision) domain.\n",
        "\n",
        "This means that we will present an image of a face to our DNN and our DNN will predict whether this image is your face or a random other face.\n",
        "\n",
        "This first step in training a DNN is making sure we have lots of high quality training data. Because collecting lots of images where the object/face is visible and lots of images where teh object/face is not visible is a lenghthy and boring task, we'll be using a couple of techniques to artifically boost our dataset.\n",
        "* We will collect a small amout of images of your own face use augmentation techniques to generate more images images for the MATCH class.\n",
        "* We will download a dataset with lots of images of faces. These images will serve as the NO-MATCH class.\n",
        "* We will download a second dataset with lots of images of object, scenes and animals to act as the NO-FACE class.\n",
        "\n",
        "To make training easier we will use a ready-made state-of-the-art architecture for image classification called [Xception](https://arxiv.org/abs/1610.02357).\n",
        "\n",
        "Finally we will download the our DNN to our phones and run inference on a realtime camera feed using TensorFlowLite.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Before we get started we will need to download the app to our phones**\n",
        "\n",
        "iOS:\n",
        "![ios.png](data:image/png;base64,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)\n",
        "Android: ![android.png](data:image/png;base64,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)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6Z01PPYZO-O"
      },
      "source": [
        "# Importing our libraries and frameworks\n",
        "\n",
        "The first thing we need to do is import the libraries and frameworks that we are going to use.\n",
        "* **Keras** -> High-level API to control TensorFlow (Deep Learning Library)\n",
        "* **Matplotlib** -> Plotting library to visually inspect our input images\n",
        "* **Augmentor** -> Used for augmenting the images in our dataset (uses PIL)\n",
        "* **QRCode** -> Library for generating a QR-code\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQsJuuXuYCJn"
      },
      "source": [
        "!pip install qrcode Augmentor > /dev/null\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import os, zipfile, qrcode, math, base64, IPython, Augmentor, requests, glob, json\n",
        "from PIL import Image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usrIBfEsYlrX"
      },
      "source": [
        "# Downloading NO-FACE and NO-MATCH images\n",
        "\n",
        "The first thing we will do is download our reference images which we will use as the NO-FACE class. We are downloading the images from a publically available dataset called [Common Objects In Context (COCO)](https://cocodataset.org/#home). This dataset contains lots of images of everyday objects and scenes. Some of the images also contain people. We remove these images since they might confuse our DNN.\n",
        "\n",
        "The NO-MATCH images we will download from the [Labeled Faces in the Wild (LFW)](http://vis-www.cs.umass.edu/lfw/) dataset. This dataset contains images of faces of different persons with different ethnicities and backgrounds."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VEc-D3IBXOiX"
      },
      "source": [
        "!rm -rf lfw val2017 data/no-face data/no-match val2017.zip lfw.tgz annotations_trainval2017.zip annotations\n",
        "!mkdir data\n",
        "\n",
        "!wget http://images.cocodataset.org/zips/val2017.zip\n",
        "!unzip val2017.zip > /dev/null\n",
        "!wget http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n",
        "!unzip annotations_trainval2017.zip > /dev/null\n",
        "\n",
        "with open(\"annotations/person_keypoints_val2017.json\", \"r\") as f:\n",
        "  annotations = json.load(f)\n",
        "\n",
        "for annotation in annotations[\"annotations\"]:\n",
        "  image_id = annotation[\"image_id\"]\n",
        "  file_name = next(x[\"file_name\"] for x in annotations[\"images\"] if x[\"id\"] == image_id)\n",
        "  if os.path.exists(f\"val2017/{file_name}\"):\n",
        "    os.remove(f\"val2017/{file_name}\") \n",
        "\n",
        "!mv val2017 data/no-face\n",
        "\n",
        "!wget http://vis-www.cs.umass.edu/lfw/lfw.tgz\n",
        "!tar zxvf lfw.tgz > /dev/null\n",
        "\n",
        "files = glob.glob(f\"lfw/**/*.jpg\")\n",
        "os.mkdir(\"data/no-match\")\n",
        "amount = len(os.listdir(\"data/no-face\"))\n",
        "for file in files[:amount]:\n",
        "  filename = os.path.basename(file)\n",
        "  os.rename(file, f\"data/no-match/{filename}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xVHs_jIYrhB"
      },
      "source": [
        "# Adding and Augmenting our own images\n",
        "\n",
        "The next step is to upload our own images. We need to take at least 25 pictures (the more the better) of our face. Getting good quality data is by far the most important step so feel free to take your time to take pictures in different locations with different lighting situations.\n",
        "\n",
        "To upload these images we simply need to create a zip file called 'data.zip' and upload this file into Colab before running the next block.\n",
        "\n",
        "We can upload files into Colab by clicking the files tab on the left and either dragging the zip file in or using the upload button. It takes a while to upload files into Colab, you can see the progress in the bottom left corner.\n",
        "\n",
        "After the images are loaded into Colab, we need to digitally augment the images. We will create these augmented images by running our original images through an augmentation pipeline. In this pipeline random augmentations will be added. Using this pipeline we will create images that all look differently."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "llf0WCeRWXuF"
      },
      "source": [
        "!rm -rf original data/match\n",
        "if not os.path.exists(\"data.zip\"):\n",
        "  raise Exception(\"Please upload a data.zip file containing your images\")\n",
        "\n",
        "with zipfile.ZipFile(\"data.zip\", \"r\") as zip_obj:\n",
        "  images = zip_obj.namelist()\n",
        "  images = [ x for x in images if x.endswith(\".jpg\") or x.endswith(\".png\") ]\n",
        "  images = [ x for x in images if not \"__MACOSX\" in x ]\n",
        "  if len(images) < 25:\n",
        "    raise Exception(\"Please make sure data.zip contains at least 25 images (jpg or png)\")\n",
        "  for image in images:\n",
        "    zip_obj.extract(image, f\"original\")\n",
        "    data = Image.open(f\"original/{image}\")\n",
        "    data.thumbnail([512, 512], Image.LANCZOS)\n",
        "    data.save(f\"original/{image}\", \"JPEG\")\n",
        "\n",
        "amount = len(os.listdir(\"data/no-match\"))\n",
        "p = Augmentor.Pipeline(\"original\", output_directory=\"../data/match\")\n",
        "p.random_distortion(probability=0.2, grid_width=4, grid_height=4, magnitude=8)\n",
        "p.random_contrast(probability=0.2, min_factor=0.5, max_factor=1)\n",
        "p.random_color(probability=0.2, min_factor=0.5, max_factor=1)\n",
        "p.random_brightness(probability=0.2, min_factor=0.5, max_factor=1)\n",
        "p.sample(amount)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyi2_rqmYzKC"
      },
      "source": [
        "# Creating our data loaders\n",
        "\n",
        "While training, we are going to need to dynamically load the images that we are going to train on (because the whole dataset does not fit in memory in one go).\n",
        "\n",
        "To do this we are going to use a Keras DataGenerator. This class will handle all the IO and preprocessing and batching for us. We are going to be working with batches of 128 images. This means we will train on 128 images at the same to to speed up te training process.\n",
        "\n",
        "As you can see in the block below we are going to create two generators. The first generator contains the images that we are going to train our DNN on. The second generator contains our validation images. We will use the validation images to make sure our DNN is not overfitting. For this it is important that the training images and the validation images are kept strictly separate. The result on the validation images will give us an idea of how well our DNN can generalize to images it has never seen before."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wp21XLkDgEnJ"
      },
      "source": [
        "data_gen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=360,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    validation_split=0.1\n",
        ")\n",
        "\n",
        "print(\"Training data:\")\n",
        "train_gen = data_gen.flow_from_directory(\n",
        "    \"data\",\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    subset=\"training\"\n",
        ")\n",
        "print()\n",
        "\n",
        "print(\"Validation data:\")\n",
        "val_gen = data_gen.flow_from_directory(\n",
        "    \"data\",\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    subset=\"validation\"\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EEZ4YqOMY1dc"
      },
      "source": [
        "# Inspecting our input data\n",
        "\n",
        "It is good practice to inspect our input data before we start training. This next block will show 25 images. Underneath each image we see the label of the class that this image belongs to.\n",
        "\n",
        "These labels are [one-hot encoded](https://en.wikipedia.org/wiki/One-hot). Which means that it is a list of integers of which only one is equal to '1'. The index of the '1' integer relates to the class this image belongs to. This means that if we see [1, 0, 0] this image belongs to the first class (MATCH), if we see [0, 1, 0] it belongs to the second class (NO-FACE) and if we see [0, 0, 1] this image belongs to the third class (NO-MATCH).\n",
        "\n",
        "If there is something wrong with the way we load our input images we can instantly see it here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSJfSarFlPku"
      },
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "batch = train_gen.next()\n",
        "for i in range(25):\n",
        "  plt.subplot(5, 5, i+1)\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "  plt.grid(False)\n",
        "  plt.imshow(batch[0][i], cmap=plt.cm.binary)\n",
        "  plt.xlabel(batch[1][i])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9KLSgPPY56q"
      },
      "source": [
        "# Creating our DNN\n",
        "\n",
        "To speed up training and it is often beneficial to employ a technique called [Transfer Learning](https://en.wikipedia.org/wiki/Transfer_learning). Transfer Learning means that you take an already trained model, in this case Google's [Xception](https://arxiv.org/abs/1610.02357), and make it specific for your dataset.\n",
        "\n",
        "The main advantange of using Transfer Learning is that you can get by with smaller datasets. Using Transfer Learning also speeds up training and means you do not have to make up your own model architecture.\n",
        "\n",
        "We will take the pretrained base model and add our own final layers to it. We are going to add two different layers.\n",
        "* A pooling layer\n",
        "* A dense (fully-connected) layer with Softmax activation.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYdZaOzanYtP"
      },
      "source": [
        "base = tf.keras.applications.Xception(input_shape=(224, 224, 3), weights=\"imagenet\", include_top=False)\n",
        "base.trainable = False\n",
        "pool_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
        "predict_layer = tf.keras.layers.Dense(train_gen.num_classes, activation=\"softmax\")\n",
        "model = tf.keras.Sequential([base, pool_layer, predict_layer])\n",
        "model.summary()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KvOqqqQNY99F"
      },
      "source": [
        "# Training our DNN on the input data\n",
        "\n",
        "Now it is time to start the training. For training we are going to need two things.\n",
        "* A loss function which is a mathematical function that determines how well our DNN is performing\n",
        "* An optimizer that will incrementally improve the DNN by calculating which parameters need to be tweaked to lower the loss value. This is done using backpropagation. \n",
        "\n",
        "During training Keras will give us two metrics. The value of the loss and the accuracy of the DNN.\n",
        "* The loss value is an arbirary value which indicates how well the DNN is performing (the lower the better)\n",
        "* Since we are doing a classification task we can calculate the amount of correct predictions and divide that by the total amount of predictions. This will give us the accuracy, a percentage of correctly predicted images (between 0 and 1).\n",
        "\n",
        "We will see both the loss and accuracy during training, as well as during validation. As mentioned before the validation images are used to see if our DNN is learning something and is able to generelize what it learned to images it has never seen before.\n",
        "\n",
        "We are going to be training the DNN for 25 epochs. Each epoch has 25 training steps and each training step has 32 images. This means in total we will be training our DNN on 20,000 different images.\n",
        "\n",
        "The validation metrics is what we are going to be looking at to see if the DNN is still learning. As long as the validation loss is still going down (or validation accuracy up) we know the DNN is still learning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UbdW0G9tpICL"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Nadam(learning_rate=2e-5)\n",
        "loss = tf.keras.losses.categorical_crossentropy\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=[\"accuracy\"])\n",
        "\n",
        "model.fit(\n",
        "    train_gen,\n",
        "    epochs=25,\n",
        "    steps_per_epoch=25,\n",
        "    validation_data=val_gen,\n",
        "    validation_steps=5\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGk1UcwvZC1D"
      },
      "source": [
        "# Converting our DNN to TFLite format\n",
        "\n",
        "Lastly we need to convert our DNN to the [TensorFlowLite](https://www.tensorflow.org/lite) format. This so we can use the DNN in the mobile app.\n",
        "\n",
        "We will be transfering the model to our device using a temporary file server called [tmp.ninja](https://tmp.ninja). This next block will generate a QR-code that you can scan with the app to transfer the model file.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdIq41NqpVHK"
      },
      "source": [
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "converter.optimizations = [ tf.lite.Optimize.OPTIMIZE_FOR_SIZE ]\n",
        "model_data = converter.convert()\n",
        "\n",
        "files = {\"files[]\": model_data }\n",
        "response = requests.post(\"https://tmp.ninja/upload.php\", files=files)\n",
        "file_info = response.json()[\"files\"][0]\n",
        "hash = file_info[\"hash\"]\n",
        "id = file_info[\"url\"].split(\"/\")[-1]\n",
        "\n",
        "img = qrcode.make(f\"ml-classification://model?id={id}&hash={hash}\")\n",
        "img.save(\"qr.png\")\n",
        "IPython.display.Image(open(\"qr.png\",\"rb\").read())"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
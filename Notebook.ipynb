{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "MLClassifier.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPv8ylfbEtNCMpdZoXfph/E"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1WEX0BsYhf-"
      },
      "source": [
        "# Welcome\n",
        "\n",
        "In this notebook we are going to train our own [Deep Neural Network (DNN)](https://en.wikipedia.org/wiki/Deep_learning).\n",
        "\n",
        "With our DNN we are going to be solving a [Categorical Classification](https://en.wikipedia.org/wiki/Statistical_classification) problem in the [Computer Vision](https://en.wikipedia.org/wiki/Computer_vision) domain.\n",
        "\n",
        "This means that we will present an image of a face to our DNN and our DNN will predict whether this image is your face or a random other face.\n",
        "\n",
        "This first step in training a DNN is making sure we have lots of high quality training data. Because collecting lots of images where the object/face is visible and lots of images where teh object/face is not visible is a lenghthy and boring task, we'll be using a couple of publically available datasets.\n",
        "* We will download a dataset with lots of images of faces. These images will serve as the FACE class.\n",
        "* We will download a second dataset with lots of images of object, scenes and animals to act as the NO-FACE class.\n",
        "\n",
        "To make training easier we will use a ready-made state-of-the-art architecture for image classification called [Xception](https://arxiv.org/abs/1610.02357).\n",
        "\n",
        "Finally we will download our DNN to our phones and run inference on a realtime camera feed using TensorFlowLite.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Before we get started we will need to download the app to our phones**\n",
        "\n",
        "iOS:\n",
        "![ios.png](data:image/png;base64,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)\n",
        "Android: ![android.png](data:image/png;base64,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)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6Z01PPYZO-O"
      },
      "source": [
        "# Importing our libraries and frameworks\n",
        "\n",
        "The first thing we need to do is import the libraries and frameworks that we are going to use.\n",
        "* **Keras** -> High-level API to control TensorFlow (Deep Learning Library)\n",
        "* **Matplotlib** -> Plotting library to visually inspect our input images\n",
        "* **QRCode** -> Library for generating a QR-code\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQsJuuXuYCJn"
      },
      "source": [
        "!pip install qrcode > /dev/null\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import os, zipfile, qrcode, math, base64, IPython, requests, glob, json\n",
        "from PIL import Image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usrIBfEsYlrX"
      },
      "source": [
        "# Downloading FACE and NO-FACE images\n",
        "\n",
        "The first thing we will do is download our reference images which we will use as the NO-FACE class. We are downloading the images from a publically available dataset called [Common Objects In Context (COCO)](https://cocodataset.org/#home). This dataset contains lots of images of everyday objects and scenes. Some of the images also contain people. We remove these images since they might confuse our DNN.\n",
        "\n",
        "The FACE images we will download from the [Labeled Faces in the Wild (LFW)](http://vis-www.cs.umass.edu/lfw/) dataset. This dataset contains images of faces of different persons with different ethnicities and backgrounds."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VEc-D3IBXOiX"
      },
      "source": [
        "!rm -rf lfw val2017 data val2017.zip lfw.tgz annotations_trainval2017.zip annotations\n",
        "!mkdir data\n",
        "\n",
        "!wget http://images.cocodataset.org/zips/val2017.zip\n",
        "!unzip val2017.zip > /dev/null\n",
        "!wget http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n",
        "!unzip annotations_trainval2017.zip > /dev/null\n",
        "\n",
        "with open(\"annotations/person_keypoints_val2017.json\", \"r\") as f:\n",
        "  annotations = json.load(f)\n",
        "\n",
        "for annotation in annotations[\"annotations\"]:\n",
        "  image_id = annotation[\"image_id\"]\n",
        "  file_name = next(x[\"file_name\"] for x in annotations[\"images\"] if x[\"id\"] == image_id)\n",
        "  if os.path.exists(f\"val2017/{file_name}\"):\n",
        "    os.remove(f\"val2017/{file_name}\") \n",
        "\n",
        "!mv val2017 data/no-face\n",
        "\n",
        "!wget http://vis-www.cs.umass.edu/lfw/lfw.tgz\n",
        "!tar zxvf lfw.tgz > /dev/null\n",
        "\n",
        "files = glob.glob(f\"lfw/**/*.jpg\")\n",
        "os.mkdir(\"data/face\")\n",
        "amount = len(os.listdir(\"data/no-face\"))\n",
        "for file in files[:amount]:\n",
        "  filename = os.path.basename(file)\n",
        "  os.rename(file, f\"data/face/{filename}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyi2_rqmYzKC"
      },
      "source": [
        "# Creating our data loaders\n",
        "\n",
        "While training, we are going to need to dynamically load the images that we are going to train on (because the whole dataset does not fit in memory in one go).\n",
        "\n",
        "To do this we are going to use a Keras DataGenerator. This class will handle all the IO and preprocessing and batching for us. We are going to be working with batches of 128 images. This means we will train on 128 images at the same to to speed up te training process.\n",
        "\n",
        "As you can see in the block below we are going to create two generators. The first generator contains the images that we are going to train our DNN on. The second generator contains our validation images. We will use the validation images to make sure our DNN is not overfitting. For this it is important that the training images and the validation images are kept strictly separate. The result on the validation images will give us an idea of how well our DNN can generalize to images it has never seen before."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wp21XLkDgEnJ"
      },
      "source": [
        "data_gen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=360,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    validation_split=0.1\n",
        ")\n",
        "\n",
        "print(\"Training data:\")\n",
        "train_gen = data_gen.flow_from_directory(\n",
        "    \"data\",\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    subset=\"training\"\n",
        ")\n",
        "print()\n",
        "\n",
        "print(\"Validation data:\")\n",
        "val_gen = data_gen.flow_from_directory(\n",
        "    \"data\",\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    subset=\"validation\"\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EEZ4YqOMY1dc"
      },
      "source": [
        "# Inspecting our input data\n",
        "\n",
        "It is good practice to inspect our input data before we start training. This next block will show 25 images. Underneath each image we see the label of the class that this image belongs to.\n",
        "\n",
        "These labels are [one-hot encoded](https://en.wikipedia.org/wiki/One-hot). Which means that it is a list of integers of which only one is equal to '1'. The index of the '1' integer relates to the class this image belongs to. This means that if we have [1, 0] this image belongs to the first class (FACE) and if we have [0, 1] it belongs to the second class (NO-FACE).\n",
        "\n",
        "In the figure below we will do a simple translation to make the class more readable. We will transform [1, 0] into FACE and [0, 1] into NO-FACE.\n",
        "\n",
        "If there is something wrong with the way we load our input images we can instantly see it here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSJfSarFlPku"
      },
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "batch = train_gen.next()\n",
        "for i in range(25):\n",
        "  plt.subplot(5, 5, i+1)\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "  plt.grid(False)\n",
        "  plt.imshow(batch[0][i], cmap=plt.cm.binary)\n",
        "  true = \"NO-FACE\" if batch[1][i][1] else \"FACE\"\n",
        "  plt.xlabel(true)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9KLSgPPY56q"
      },
      "source": [
        "# Creating our DNN\n",
        "\n",
        "To speed up training and it is often beneficial to employ a technique called [Transfer Learning](https://en.wikipedia.org/wiki/Transfer_learning). Transfer Learning means that you take an already trained model, in this case Google's [Xception](https://arxiv.org/abs/1610.02357), and make it specific for your dataset.\n",
        "\n",
        "The main advantange of using Transfer Learning is that you can get by with smaller datasets. Using Transfer Learning also speeds up training and means you do not have to make up your own model architecture.\n",
        "\n",
        "We will take the pretrained base model and add our own final layers to it. We are going to add two different layers.\n",
        "* A pooling layer\n",
        "* A dense (fully-connected) layer with Softmax activation.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYdZaOzanYtP"
      },
      "source": [
        "base = tf.keras.applications.Xception(input_shape=(224, 224, 3), weights=\"imagenet\", include_top=False)\n",
        "base.trainable = False\n",
        "pool_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
        "predict_layer = tf.keras.layers.Dense(train_gen.num_classes, activation=\"softmax\")\n",
        "model = tf.keras.Sequential([base, pool_layer, predict_layer])\n",
        "model.summary()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KvOqqqQNY99F"
      },
      "source": [
        "# Training our DNN on the input data\n",
        "\n",
        "Now it is time to start the training. For training we are going to need two things.\n",
        "* A loss function which is a mathematical function that determines how well our DNN is performing\n",
        "* An optimizer that will incrementally improve the DNN by calculating which parameters need to be tweaked to lower the loss value. This is done using backpropagation. \n",
        "\n",
        "During training Keras will give us two metrics. The value of the loss and the accuracy of the DNN.\n",
        "* The loss value is an arbirary value which indicates how well the DNN is performing (the lower the better)\n",
        "* Since we are doing a classification task we can calculate the amount of correct predictions and divide that by the total amount of predictions. This will give us the accuracy, a percentage of correctly predicted images (between 0 and 1).\n",
        "\n",
        "We will see both the loss and accuracy during training, as well as during validation. As mentioned before the validation images are used to see if our DNN is learning something and is able to generelize what it learned to images it has never seen before.\n",
        "\n",
        "We are going to be training the DNN for 25 epochs. Each epoch has 25 training steps and each training step has 32 images. This means in total we will be training our DNN on 20,000 different images.\n",
        "\n",
        "The validation metrics is what we are going to be looking at to see if the DNN is still learning. As long as the validation loss is still going down (or validation accuracy up) we know the DNN is still learning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UbdW0G9tpICL"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Nadam(learning_rate=2e-5)\n",
        "loss = tf.keras.losses.categorical_crossentropy\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=[\"accuracy\"])\n",
        "\n",
        "model.fit(\n",
        "    train_gen,\n",
        "    epochs=25,\n",
        "    steps_per_epoch=25,\n",
        "    validation_data=val_gen,\n",
        "    validation_steps=5\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYUYBIQ7X9xl"
      },
      "source": [
        "# Testing our DNN\n",
        "\n",
        "A logical next step is to visually inspect whether our DNN is giving valid predictions. Next we will load some of the validation images and have our DNN predict whether there is a person or not in the image (remember our DNN has not seen the validation set during training and therefore we can use this subset for testing the DNN).\n",
        "\n",
        "In practice the app that we are going to use does exactly the same as the following code, except for that it uses images that have been captured in realtime using the device's camera.\n",
        "\n",
        "The output that the model will give of each image is an array of floating point values that add up to 1. These values are the confidences for each class. An example of an output could be [0.83, 0.17]. This means that the model is 83% confident the input image belongs to the class on the first index (which in our case is the FACE class). The higher the confidence is for a given class the more sure the model is that this image belongs to that respective class.\n",
        "\n",
        "We will do a couple of transformations to our output to make it more readable. The output we will see below explains the prediction that the model has made. In the figure you will see a couple of things:\n",
        "* The input image that is fed into the model.\n",
        "* The prediction (either FACE or NO-FACE). \n",
        "* The confidence of the prediction (value between 0 and 1)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AaiC5W8pYmmF"
      },
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "batch = val_gen.next()\n",
        "batch_y = model(batch[0])\n",
        "batch_pred = tf.math.argmax(batch_y, axis=-1)\n",
        "batch_prob = tf.math.reduce_max(batch_y, axis=-1)\n",
        "for i in range(25):\n",
        "  plt.subplot(5, 5, i+1)\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "  plt.grid(False)\n",
        "  plt.imshow(batch[0][i], cmap=plt.cm.binary)\n",
        "  pred = \"NO-FACE\" if batch_class[i] else \"FACE\"\n",
        "  prob = tf.cast(batch_prob[i] * 100, tf.int32)\n",
        "  plt.xlabel(f\"{pred} ({prob}%)\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGk1UcwvZC1D"
      },
      "source": [
        "# Converting our DNN to TFLite format\n",
        "\n",
        "Lastly we need to convert our DNN to the [TensorFlowLite](https://www.tensorflow.org/lite) format. This so we can use the DNN in the mobile app.\n",
        "\n",
        "We will be transfering the model to our device using a temporary file server called [tmp.ninja](https://tmp.ninja). This next block will generate a QR-code that you can scan with the app to transfer the model file.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdIq41NqpVHK"
      },
      "source": [
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "converter.optimizations = [ tf.lite.Optimize.DEFAULT ]\n",
        "model_data = converter.convert()\n",
        "\n",
        "files = {\"files[]\": model_data }\n",
        "response = requests.post(\"https://tmp.ninja/upload.php?output=text\", files=files)\n",
        "file_info = response.json()[\"files\"][0]\n",
        "hash = file_info[\"hash\"]\n",
        "id = file_info[\"url\"].split(\"/\")[-1]\n",
        "\n",
        "img = qrcode.make(f\"ml-classification://model?id={id}&hash={hash}\")\n",
        "img.save(\"qr.png\")\n",
        "IPython.display.Image(open(\"qr.png\",\"rb\").read())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvrTMzjmexxL"
      },
      "source": [
        "# What's Next?\n",
        "\n",
        "Now that we have trained our own model we can start thinking about the next steps. Of course we want our model to make the best predictions as possible. There are two things we can work on to improve our model.\n",
        "\n",
        "The first and most important one is the data. The better training data we have the our final model is going to be. The more variation there is in our data the better our model will be at generalizing to images it has never seen before. Not only quantity of data is important here but also the quality of the data.\n",
        "\n",
        "The second thing we can work on is tweaking the hyperparameters and the model. We can change the architecture of the model or alter parameters such as the learning-rate, the optimizer and the loss function to improve the model. Remember that tweaks like this might take your model from 98% accuracy to 99% but if your training data and validation data is bad changing the architecture or hyperparameters is not going to make your model generalize better.\n",
        "\n",
        "Another thing that we might try to do is make our model explain its predictions. Right now we have no idea why the model is classifying some images as FACE and others as NO-FACE. There are several methods that we can employ: \n",
        "* Ceating heatmaps which reveal areas of the image that contributed most to the prediction of the model.\n",
        "* Looking at intermediate activations of the model to deduce features that the model looks at.\n",
        "* Generating adverserial examples to see which types of images result in a high probability in one output class.\n",
        "* Adding a explainer DNN to our predictor DNN that can describe what the prediction DNN is predicting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCzdrLdhTAqM"
      },
      "source": [
        "# Testing without running the scripts\n",
        "\n",
        "If you would like to test the app without training your own model you can download a pre-trained DNN right here. Just scan the following QR-code using the app.\n",
        "\n",
        "![QR](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAcIAAAHCAQAAAABUY/ToAAADe0lEQVR4nO2cS6rjSBBFT3QKPExBL6CXkt7Zo5ZUO5CW4h1IQ4NM9CA/Stmmaap5bbu4d+CPrIMlCCLihlIy59c0//GLIIgUKVKkSJEiRb4faUUDwC1/sjM3g7X9sNa9zi8+WpFvSSZ3d18A1gGf4ob/GIMzj8F94mZAcHd3P5KvOFqRb0UO5X0dIf0EA7B0GTZLl2GDuGBp+XMDwCBsLzxake9IDs83x6v5/NfVPC23gXnEHmYAn3WeIv9XMjisJ7evywCzDbi7u52/8z9FfjCZA6T2Q8FJS/1himWbT3GDtNR+KLm7T591niK/nZzNzGwEIDhpCTkP2Rmwr8vJKYbN7PVHK/KtyJyHdkF0d/cte7X9691+ykMim0p5WoDi3jdqBfOckXwiVOdfDb5iSGRV78vSBMxngDwQMhyulg0+8WjNPus8RX4fWWrURHCf8teah9pL3jGXNoJ6apF3qlZra9FULFlfxrwMFssPiiGRverli9CSTN9Ok5bgEDd8ojh/9UMijyoREb1voksZgxw+eXCUi5xiSOS9uhgqvU8tXn0DVMInb1MtE3lQ9falZEH18fmls/XHSZFiSGRVGfcssPsyuvaoBVdagnyZyGfqe+rY7BfN5Xtpp/OaolzzFEMiH8iyymw9tXE02Hk9udkYHOLV7LzWxYznFx+tyPci++tgU2zjxW5cVItXzkixNNvKQyKbaj8U2lARSlWLd802aD4k8on2NNSlm5aMynzI6yhb/ZDIR5Weus6H9itnJQ/VIpcv7ecXxZDIXrsve8hI7TLHMTcpD4l8QuZMwzpQ7Fde+wHulwHmkbzNzE51ZfUnnqfI7yDz+iEj3gYnbvg85vuDsPTzZhC3weDk+/ohf93RinxHsluTn03XFA+2ntYeJW/mTLVMZK97Szbttr5+7ZvtqH5I5L1KDC3HdBPddx+v+ZDIf5QflbdN0f2wS10K0m4yUwyJbNojB2q1Kjomo+4KvmJIZK/7gVCuVgvQFoD0bXdLWIohkVU1ydBWCNEWv/aNdSflIZG9nj73w9J0NZ8NnHWkzIeAcqfZf/xPkb85mRbwHzZgX0v/DKvi/OsA+02OVuQbkDUPRQdW8HkMm6VlxFlv+alDlpY6vIbgykMij7rzZfk9tUXVbRV1vfUD9UMij3p8vtm/lJ5xLlKkSJEiRYr8Tci/ASPlzOJVPLj/AAAAAElFTkSuQmCC)\n"
      ]
    }
  ]
}